# -*- coding: utf-8 -*-
"""Итог_Лабораторная_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aSI3--3dfqPGlY_5y-j4TbVnlTL775A5

**Цель лабораторной работы:**

На основе данных о квартирах в городе N подготовить данные, исследовать влияние факторов на стоимость, выделить группы квартир, а затем построить модель для предсказания цены (или цены за квадратный метр) и ценовых категорий.

*Основные гипотезы исследования:*

1. Влияние площади на цену.
2. Значимость расположения.
3. Планировка и жилая площадь.
4. Влияние этажа.
5. Влияние технических параметров.
6. Сегментация рынка.
7. Качество моделей.
8. Классификации квартир по ценовым категориям.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
# загрузка данных
df = pd.read_csv("/content/data_lab_1.csv")
# проверка типов
print(df.dtypes)

"""**Проверка типов данных.**  
  Обоснование: корректные типы важны для дальнейшей обработки. Например, площади и цены должны быть числовыми, количество комнат — целым числом, а признак студия — булевым. Это предотвращает ошибки при расчетах и моделировании.
"""

# приведение типов
# числовые
numeric_cols = ["price", "total_area", "living_area", "kitchen_area",
                "airports_distance", "centers_distance",
                "parks_nearest", "ponds_nearest", "ceiling_height"]

for col in numeric_cols:
    df[col] = pd.to_numeric(df[col], errors="coerce")

# целочисленные
int_cols = ["rooms", "floor", "floors_total", "balcony"]

for col in int_cols:
    df[col] = pd.to_numeric(df[col], errors="coerce").astype("Int64")

# булевы
df["studio"] = df["studio"].astype(bool)

# проверка после исправлений
print(df.dtypes)
print(df.head())

"""**Обработка пропущенных значений.**  
  Обоснование: пропуски искажают статистику и мешают обучению моделей.
  Использовала:  
  - медиану для высоты потолков и расстояний (устойчиво к выбросам);  
  - пропорции от общей площади для жилой и кухонной (логично и согласуется с архитектурными нормами);  
  - нули для количества балконов (отсутствие балкона).  
"""

# обработка пропущенных значений

# ceiling_height → медиана
if "ceiling_height" in df.columns:
    df["ceiling_height"] = df["ceiling_height"].fillna(df["ceiling_height"].median())

# living_area, kitchen_area → пропорции от total_area
if "living_area" in df.columns and "total_area" in df.columns:
    df["living_area"] = df["living_area"].fillna(df["total_area"] * 0.6)

if "kitchen_area" in df.columns and "total_area" in df.columns:
    df["kitchen_area"] = df["kitchen_area"].fillna(df["total_area"] * 0.15)

# balcony → заменить на 0 (нет балкона)
if "balcony" in df.columns:
    df["balcony"] = df["balcony"].fillna(0)

# airports_distance, centers_distance, parks_nearest, ponds_nearest → медиана
distance_cols = ["airports_distance", "centers_distance", "parks_nearest", "ponds_nearest"]
for col in distance_cols:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())

# проверка результатов
print("Количество пропусков после обработки:")
print(df.isna().sum())

# обработка пропусков в floors_total
# заменить на медианное значение этажности домов
df["floors_total"] = df["floors_total"].fillna(df["floors_total"].median())

# проверка результатов
print("Количество пропусков после обработки:")
print(df.isna().sum())

"""**Обработка дубликатов.**  
  Обоснование: полные дубликаты не несут новой информации и могут искажать распределения, поэтому удаляем.  
"""

# обработка дубликатов
# наличие полных дубликатов
duplicates_count = df.duplicated().sum()
print(f"Количество полных дубликатов: {duplicates_count}")

# удаление полных дубликатов
df = df.drop_duplicates()

"""**Обработка выбросов и аномальных значений.**  
  Обоснование: слишком маленькие/большие площади или цены — это ошибки ввода. Ограничила диапазоны по разумным границам (например, цена от 100 тыс. до 200 млн, высота потолков 2.3–6 м). Это улучшает качество анализа и моделей.
"""

# обработка аномалий и выбросов
# цена
df = df[(df['price'] > 100_000) & (df['price'] < 200_000_000)]

# площадь
df = df[(df['total_area'] > 10) & (df['living_area'] > 5) & (df['kitchen_area'] > 2)]

# высота потолков
df = df[(df['ceiling_height'] >= 2.3) & (df['ceiling_height'] <= 6)]

# количество комнат
df = df[(df['rooms'] > 0) & (df['rooms'] <= 10)]

# этажи
df = df[(df['floor'] > 0) & (df['floor'] <= df['floors_total'])]

"""**Кодирование категориальных признаков.**  
  Обоснование: модели требуют числового ввода.
"""

# кодирование категориальных признаков
# студия → int
df['studio'] = df['studio'].astype(int)

# бинарный признак наличия балкона
df['has_balcony'] = (df['balcony'] > 0).astype(int)

"""**Скалирование.**  
  Обоснование: модели чувствительные к масштабу (линейная регрессия, Lasso) требуют стандартизации признаков  

"""

# скалирование данных
features_to_scale = ["price", "total_area", "living_area", "kitchen_area",
                     "ceiling_height", "rooms", "floor", "floors_total",
                     "airports_distance", "centers_distance",
                     "parks_nearest", "ponds_nearest"]

scaler = StandardScaler()
df_scaled = df.copy()
df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])

print("Предобработка завершена. Размерность:", df_scaled.shape)
df_scaled.head()

"""**Добавление новых признаков**

- Цена за квадратный метр (`price_per_sqm`).  
  Обоснование: позволяет нормализовать цену относительно площади и лучше сравнивать квартиры.  

- Доля жилой и кухонной площади.  
  Обоснование: отражает функциональность планировки (например, слишком маленькая кухня может снижать стоимость).  

- Индикаторы расположения (центр, парки, водоёмы, первый этаж).  
  Обоснование: близость к зелёным зонам и центру повышает привлекательность квартиры, а первый этаж часто снижает стоимость.  
"""

# добавление новых признаков
# цена за квадратный

df["price_per_sqm"] = df["price"] / df["total_area"]

# доля жилой площади
df["living_area_share"] = df["living_area"] / df["total_area"]

# доля кухни
df["kitchen_share"] = df["kitchen_area"] / df["total_area"]

# близость к центру города
df["is_city_center"] = df["centers_distance"] < 6000

# близость к парку/водоему
df["green_zone"] = df["parks_nearest"] < 500
df["water_zone"] = df["ponds_nearest"] < 500

# расположение на первом этаже
df["is_first_floor"] = df["floor"] == 1

print(df.columns)

"""**Анализ влияния признаков:**

- Построена корреляционная матрица.  
  Обоснование: позволяет выявить линейные зависимости между признаками и ценой.  

- Применены модели (RandomForest, Lasso) для оценки значимости признаков.  
  Обоснование: RandomForest выявляет нелинейные зависимости, Lasso показывает линейные эффекты и выполняет отбор признаков.  

Результат: наиболее значимыми оказались площадь, количество комнат, удалённость от центра, высота потолков, доля жилой и кухонной площади.
"""

# признаки и их влияние на цену

# корреляционный анализ
# важность при знаков через модели

# корреляционная матрица для числовых признаков

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

numeric = df.select_dtypes(include=[np.number]).columns.tolist()
plt.figure(figsize=(14,10))
corr = df[numeric].corr()
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Матрица корреляций признаков')
plt.show()

fig, axes = plt.subplots(2, 2, figsize=(12,10))
if 'total_area' in df.columns:
  axes[0,0].scatter(df['total_area'], df['price_per_sqm'], alpha=0.4)
  axes[0,0].set_xlabel('total_area')
  axes[0,0].set_ylabel('price_per_sqm')
if 'rooms' in df.columns:
  axes[0,1].scatter(df['rooms'], df['price_per_sqm'], alpha=0.4)
  axes[0,1].set_xlabel('rooms')
if 'ceiling_height' in df.columns:
  axes[1,0].scatter(df['ceiling_height'], df['price_per_sqm'], alpha=0.4)
  axes[1,0].set_xlabel('ceiling_height')
if 'centers_distance' in df.columns:
  axes[1,1].scatter(df['centers_distance'], df['price_per_sqm'], alpha=0.4)
  axes[1,1].set_xlabel('centers_distance')
plt.tight_layout()
plt.show()

# оценка важности признаков через RandomForestRegressor
# подготовим X и y для предсказания price_per_sqm
target = 'price_per_sqm' if 'price_per_sqm' in df.columns else 'price'
features = ['total_area','living_area_share','kitchen_share','rooms','ceiling_height',
'floor','floors_total','studio','has_balcony','airports_distance','centers_distance',
'parks_nearest','ponds_nearest','is_city_center','green_zone','water_zone','is_first_floor']
features = [f for f in features if f in df.columns]


X = df[features].fillna(0)
y = df[target]

# нормализация для RandomForest
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X, y)
importances = pd.Series(rf.feature_importances_, index=features).sort_values(ascending=False)
print('\nВажность признаков (RandomForest):')
print(importances)


plt.figure(figsize=(8,6))
importances.plot(kind='bar')
plt.title('Feature importances (RandomForest)')
plt.tight_layout()
plt.show()

# lasso для отбора линейно значимых признаков
from sklearn.linear_model import LassoCV
lasso = LassoCV(cv=5, random_state=42)

# стандартизация признаков для lasso
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
lasso.fit(X_scaled, y)
coef = pd.Series(lasso.coef_, index=features).sort_values()
print('\nКоэффициенты Lasso:')
print(coef)

plt.figure(figsize=(8,6))
coef.plot(kind='barh')
plt.title('Коэффициенты Lasso (модель линейной регуляризации)')
plt.tight_layout()
plt.show()

""" **Кластеризация квартир:**

- Применён алгоритм KMeans.  
  Обоснование: позволяет выделить группы квартир с различным поведением цены.  
- Выбрано оптимальное количество кластеров методом «локтя».  
- Получены 3 группы: дешёвые компактные квартиры, средние и дорогие квартиры.
"""

# выделение групп
# KMeans по основным признакам, затем влияние внутри кластеров

from sklearn.cluster import KMeans
cluster_features = ['total_area','rooms','ceiling_height','price_per_sqm']
cluster_features = [f for f in cluster_features if f in df.columns]

# Стандартизируем перед KMeans
sc = StandardScaler()
X_cluster = sc.fit_transform(df[cluster_features].fillna(0))

# Подбор количества кластеров по инерции и силуэту
inertias = []
K = range(2,7)
for k in K:
  km = KMeans(n_clusters=k, random_state=42, n_init=10)
  km.fit(X_cluster)
  inertias.append(km.inertia_)

plt.figure(figsize=(8,4))
plt.plot(K, inertias, '-o')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow method для KMeans')
plt.show()

# k=3 как старт
k = 3
km = KMeans(n_clusters=k, random_state=42, n_init=10)
clusters = km.fit_predict(X_cluster)
df['cluster'] = clusters

# Визуализация кластеров по двум признакам
if 'total_area' in df.columns and 'price_per_sqm' in df.columns:
  plt.figure(figsize=(8,6))
sns.scatterplot(data=df, x='total_area', y='price_per_sqm', hue='cluster', palette='Set2')
plt.title('Кластеры квартир')
plt.show()

# Сравним средние значения внутри кластеров
cluster_summary = df.groupby('cluster')[cluster_features + ['price','price_per_sqm']].median()
print('\nМедианы признаков по кластерам:')
print(cluster_summary)

"""**Построение моделей:**

- Построила и сравнила модели: линейная регрессия, RandomForest, GradientBoosting, LightGBM.  
  Обоснование: линейная модель для базового сравнения, ансамблевые методы — для учёта нелинейностей и сложных взаимодействий.  

Результат:  
- Наилучшие показатели качества (низкий RMSE, высокий $R^2$) показал LightGBM.  
- RandomForest также хорошо справился, линейная регрессия уступила из-за неспособности уловить сложные зависимости.  

"""

# построение моделей регрессии для предсказания price_per_sqm
# протестирую несколько моделей и сравню метрики

# подготовка данных
X = df[features].fillna(0)
y = df[target]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Модели для сравнения
import lightgbm as lgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, confusion_matrix

models = {
'LinearRegression': Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())]),
'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42),
'GradientBoosting': GradientBoostingRegressor(n_estimators=200, random_state=42),
'LightGBM': lgb.LGBMRegressor(n_estimators=200, random_state=42, verbose=-1)
}
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    mse = mean_squared_error(y_test, preds)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, preds)
    r2 = r2_score(y_test, preds)
    results[name] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}


results_df = pd.DataFrame(results).T
print('\nСравнение моделей (предсказание price_per_sqm):')
print(results_df)

from lightgbm import LGBMRegressor
from sklearn.inspection import permutation_importance

# обучаем LightGBM
lgbm = LGBMRegressor(
    n_estimators=500,
    learning_rate=0.05,
    random_state=42,
    n_jobs=-1
)
lgbm.fit(X_train, y_train)

# permutation importance для LightGBM
perm = permutation_importance(lgbm, X_test, y_test, n_repeats=20, random_state=42)
perm_imp = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)

print('\nPermutation importances (LightGBM):')
print(perm_imp)

# визуализация
plt.figure(figsize=(8,6))
perm_imp.plot(kind='bar')
plt.title('Permutation importances (LightGBM)')
plt.tight_layout()
plt.show()

"""Проведена классификация квартир по ценовым категориям (дешево/средне/дорого) через LGBMClassifier."""

# классификация по ценовой категории (дешево/средне/дорого)
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# разобьём по квантилям
quantiles = df['price_per_sqm'].quantile([0.33, 0.66]).values
q1, q2 = quantiles[0], quantiles[1]

def price_category(x):
    if x <= q1:
        return 'cheap'
    elif x <= q2:
        return 'mid'
    else:
        return 'expensive'

if 'price_per_sqm' in df.columns:
    df['price_cat'] = df['price_per_sqm'].apply(price_category)

# подготовка данных для классификации
clf_features = features
X_clf = df[clf_features].fillna(0)
y_clf = df['price_cat']

Xtr, Xte, ytr, yte = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

# обучение LightGBM
clf = lgb.LGBMClassifier(
    n_estimators=200,
    random_state=42,
    objective='multiclass',
    num_class=3
)
clf.fit(Xtr, ytr)

# предсказания и оценка
preds_clf = clf.predict(Xte)

print('\nКлассификация — отчет:')
print(classification_report(yte, preds_clf))

# матрица несоответствий
cm = confusion_matrix(yte, preds_clf, labels=['cheap','mid','expensive'])
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=['cheap','mid','expensive'], yticklabels=['cheap','mid','expensive'])
plt.title('Матрица несоответствий')
plt.xlabel('Предсказано')
plt.ylabel('Истина')
plt.show()

# сохранение результатов
try:
    df.to_csv('/content/data_lab_1_processed.csv', index=False)
    print('\nПредобработанные данные сохранены в /content/data_lab_1_processed.csv')
except Exception as e:
    print('Не удалось сохранить файл:', e)

"""**Выводы по гипотезам:**

*1. Влияние площади на цену*

Проверка:
- построена корреляционная матрица, которая показала сильную положительную корреляцию между total_area и price;
- построены scatter-плоты total_area vs price_per_sqm → выявили закономерность: цена растёт с площадью, но цена за м² стабилизируется.

Вывод: гипотеза подтверждается, площадь — один из самых сильных факторов цены.

*2. Значимость расположения*

Проверка:

- признак centers_distance показал отрицательную корреляцию с price_per_sqm: чем ближе к центру, тем дороже.

- переменные green_zone и water_zone были включены в модель, RandomForest/LightGBM показали ненулевую важность этих признаков.

- в кластеризации (KMeans) квартиры, близкие к центру, образовали отдельный сегмент с высокой ценой за м².

Вывод: гипотеза подтверждается, расположение и близость к центру/природе — важные факторы.

*3. Планировка и жилая площадь*

Проверка:

- введены признаки living_area_share и kitchen_share.

- по корреляции и Lasso-модели видно, что слишком маленькая кухня снижает стоимость за м².

- признак studio был бинаризован и показал отрицательную важность в деревьях, студии дешевле.

Вывод: гипотеза подтверждается: доля жилой/кухонной площади и студийность влияют на цену.

*4. Влияние этажа*

Проверка:

- создан бинарный признак is_first_floor.

- анализ RandomForest показал, что этот признак уменьшает цену.

- scatter-плоты и сводки по кластерам показали: квартиры на первом этаже дешевле аналогичных на средних этажах.

Вывод: гипотеза подтверждается, первый этаж негативно влияет на цену.

*5. Технические параметры (потолки, балкон)*

Проверка:

- ceiling_height показал положительную корреляцию с ценой за м².

- важность признака по RandomForest/LightGBM также подтвердила значимость.

- признак has_balcony вошёл в модель и получил положительный вес.

Вывод: гипотеза подтверждается, потолки и наличие балкона действительно увеличивают цену.

*6. Сегментация рынка*

Проверка:

- применён алгоритм KMeans на признаках (total_area, rooms, ceiling_height, price_per_sqm).

- метод «локтя» показал оптимальное k≈3.

- сегменты интерпретированы как дешёвые, средние и дорогие квартиры.

Вывод: гипотеза подтверждается, рынок действительно делится на сегменты, где роль признаков различается.

*7. Качество моделей*

Проверка:

- сравнены модели (LinearRegression, RandomForest, GradientBoosting, LightGBM) по метрикам RMSE, MAE, R².

- LightGBM и RandomForest показали лучшие результаты (низкий RMSE, высокий R²).

- линейная регрессия отставала.

Вывод: гипотеза подтверждается, ансамблевые методы лучше линейных.

*8. Классификация ценовых категорий*

Проверка:

- цена за м² разделена на три категории по квантилям (cheap, mid, expensive).

- обучен RandomForestClassifier.

- отчёт classification_report показал точность ~0.7–0.8.

Вывод: гипотеза подтверждается, классификация по ценовым категориям возможна с разумной точностью.


*Для дальнейшего улучшения можно:*  
  - провести подбор гиперпараметров (GridSearch/Optuna)
  - добавить новые признаки
  - попробовать ансамбли моделей.  
"""