# -*- coding: utf-8 -*-
"""Lab_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16hDVe62v_AzTFwiCxrCZi7IgLDT0PN6h
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import math

# 1. Загрузка данных
df = pd.read_csv("/content/credit_scoring.csv")
df.columns = [c.replace('-', '_').strip() for c in df.columns]

print("Размер данных:", df.shape)
print("\nПервые строки таблицы:")
print(df.head())

print("\nИнформация о данных:")
print(df.info())

print("\nКоличество пропусков по признакам:")
print(df.isna().sum())

print("\nСтатистические характеристики числовых признаков:")
print(df.describe())

# 2. Предобработка данных
target = 'Delinquent90'
features = [c for c in df.columns if c not in ['client_id', target]]

# Преобразуем признаки в числовой формат
for c in features:
    df[c] = pd.to_numeric(df[c], errors='coerce')

# Удаляем строки с пропусками в целевой переменной
df = df.dropna(subset=[target])

# Проверка баланса классов
print("\nБаланс классов в целевой переменной:")
print(df[target].value_counts(normalize=True))

# 3. EDA: распределения, корреляция и выбросы
numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
if target in numeric_cols:
    numeric_cols.remove(target)

# Гистограммы распределений
n_cols = 3
n_rows = math.ceil(len(numeric_cols) / n_cols)
plt.figure(figsize=(15, n_rows * 3))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(df[col], bins=20, kde=True, color="skyblue")
    plt.title(col)
    plt.xlabel("")
plt.suptitle("Распределение признаков", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# Корреляционная матрица
plt.figure(figsize=(10, 8))
sns.heatmap(df[numeric_cols + [target]].corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Корреляционная матрица")
plt.show()

# Boxplot для выявления выбросов
plt.figure(figsize=(15, n_rows * 3))
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.boxplot(x=df[col], color="lightgreen")
    plt.title(col)
plt.suptitle("Boxplot для выявления выбросов", fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

# 4. Обработка выбросов (опционально)
# Простейшая обрезка значений по 1 и 99 процентилю
for col in numeric_cols:
    lower = df[col].quantile(0.01)
    upper = df[col].quantile(0.99)
    df[col] = np.clip(df[col], lower, upper)

# 5. Масштабирование и имьютация
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

num_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
preprocessor = ColumnTransformer([('num', num_pipe, features)])

X = df[features]
y = df[target]

# 6. Разделение на train/test
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Применяем предобработку
X_train_prep = preprocessor.fit_transform(X_train)
X_test_prep = preprocessor.transform(X_test)

# 7. SMOTE (только для обучающей выборки)
from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_prep, y_train)

print("\nБаланс классов после SMOTE:")
print(pd.Series(y_train_res).value_counts(normalize=True))

# 8. Decision Tree – обучение и визуализация лучшего дерева
from sklearn.tree import DecisionTreeClassifier, plot_tree

dt = DecisionTreeClassifier(max_depth=5, random_state=42)
dt.fit(X_train_res, y_train_res)

plt.figure(figsize=(20,10))
plot_tree(dt, feature_names=features, class_names=['0','1'], filled=True)
plt.title("Визуализация дерева решений")
plt.show()

# 9. Анализ влияния глубины
train_acc, test_acc = [], []
depths = range(1, 12)
for d in depths:
    dt_temp = DecisionTreeClassifier(max_depth=d, random_state=42)
    dt_temp.fit(X_train_res, y_train_res)
    train_acc.append(dt_temp.score(X_train_res, y_train_res))
    test_acc.append(dt_temp.score(X_test_prep, y_test))

plt.figure(figsize=(8, 5))
plt.plot(depths, train_acc, label="Train Accuracy")
plt.plot(depths, test_acc, label="Test Accuracy")
plt.xlabel("Глубина дерева")
plt.ylabel("Accuracy")
plt.title("Влияние глубины дерева решений")
plt.legend()
plt.grid(True)
plt.show()

best_depth = depths[np.argmax(test_acc)]
print(f"\nОптимальная глубина дерева: {best_depth}")

# 10. GridSearchCV для Decision Tree
from sklearn.model_selection import train_test_split, GridSearchCV

param_grid_dt = {
    'max_depth': [8, 9, 10, 11, 12],  # диапазон вокруг найденной оптимальной глубины
    'criterion': ['gini', 'entropy'],
    'min_samples_leaf': [1, 5, 10]
}

grid_dt = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid_dt,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Обучаем на данных после SMOTE
grid_dt.fit(X_train_res, y_train_res)
best_dt = grid_dt.best_estimator_

print("\nЛучшие параметры дерева решений:", grid_dt.best_params_)
print("Accuracy на тесте:", best_dt.score(X_test_prep, y_test))

# Bagging с Logistic Regression
from sklearn.ensemble import BaggingClassifier
from sklearn.linear_model import LogisticRegression

bag = BaggingClassifier(
    estimator=LogisticRegression(max_iter=1000, solver='liblinear'),
    random_state=42
)

param_grid_bag = {
    'n_estimators': [10, 30, 50],
    'max_samples': [0.5, 0.8, 1.0],
    'max_features': [0.5, 0.8, 1.0],
    'estimator__C': [0.1, 1, 10]
}

grid_bag = GridSearchCV(bag, param_grid_bag, cv=5, scoring='accuracy', n_jobs=-1)
grid_bag.fit(X_train_res, y_train_res)

best_bag = grid_bag.best_estimator_

print("\nЛучшие параметры Bagging:", grid_bag.best_params_)
print("Accuracy на тесте:", best_bag.score(X_test_prep, y_test))

# 12.Stacking с финальной Logistic Regression
from sklearn.ensemble import StackingClassifier
from sklearn.neighbors import KNeighborsClassifier

# Базовые модели (разнообразные по типу)
estimators = [
    ('knn', KNeighborsClassifier()),
    ('dt', DecisionTreeClassifier(max_depth=3)),
    ('lr', LogisticRegression(max_iter=1000, solver='liblinear'))
]

# Финальная модель — Logistic Regression
stack = StackingClassifier(
    estimators=estimators,
    final_estimator=LogisticRegression(max_iter=1000, solver='liblinear'),
    cv=5,
    n_jobs=-1
)

# Подбор параметров для стекинга
param_grid_stack = {
    'final_estimator__C': [0.1, 1, 10],
    'knn__n_neighbors': [3, 5, 7],
    'dt__max_depth': [2, 3, 5]
}

grid_stack = GridSearchCV(
    stack,
    param_grid_stack,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_stack.fit(X_train_res, y_train_res)
best_stack = grid_stack.best_estimator_

print("\nЛучшие параметры Stacking:", grid_stack.best_params_)
print("Accuracy на тесте:", best_stack.score(X_test_prep, y_test))

# 13. Random Forest
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_rf = GridSearchCV(
    rf,
    param_grid_rf,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_rf.fit(X_train_res, y_train_res)
best_rf = grid_rf.best_estimator_

print("\nЛучшие параметры RandomForest:", grid_rf.best_params_)
print("Accuracy на тесте:", best_rf.score(X_test_prep, y_test))

# 13. XGBoost
try:
    from xgboost import XGBClassifier
    xgb = XGBClassifier(
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        eval_metric='logloss',
        random_state=42
    )
except ImportError:
    from sklearn.ensemble import HistGradientBoostingClassifier
    xgb = HistGradientBoostingClassifier(max_depth=5, random_state=42)

xgb.fit(X_train_res, y_train_res)
print("\nXGBoost Accuracy:", xgb.score(X_test_prep, y_test))

# 14. F-test (ANOVA)
from sklearn.feature_selection import f_classif

F, p = f_classif(X_train_prep, y_train)
f_test = pd.DataFrame({'Feature': features, 'F': F, 'p-value': p})
print("\nF-test (ANOVA):")
print(f_test.sort_values('F', ascending=False))

# 15. Сравнение моделей
results = pd.DataFrame({
    'Model': ['Decision Tree', 'Bagging', 'Stacking', 'Random Forest', 'XGBoost'],
    'Accuracy': [
        best_dt.score(X_test_prep, y_test),
        best_bag.score(X_test_prep, y_test),
        best_stack.score(X_test_prep, y_test),
        best_rf.score(X_test_prep, y_test),
        xgb.score(X_test_prep, y_test)
    ]
})

print("\nСравнение моделей:")
print(results.sort_values('Accuracy', ascending=False))

plt.figure(figsize=(7, 4))
sns.barplot(data=results, x='Model', y='Accuracy', palette='viridis')
plt.title("Сравнение точности моделей")
plt.xticks(rotation=30)
plt.show()

"""**Отчёт по лабораторной работе: Анализ, предобработка данных и классификация клиентов кредитного скоринга**

*Цель работы*

Освоить этапы анализа данных (EDA), их предобработки и балансировки, а также изучить и сравнить эффективность различных алгоритмов классификации, включая ансамблевые методы (бэггинг, стекинг, случайный лес и XGBoost).

**1. Анализ исходных данных**

*Источник данных:* файл credit_scoring.csv (75 000 записей, 11 признаков).
*Целевая переменная: *Delinquent90 — наличие просрочки платежа более 90 дней (1 — есть, 0 — нет).

Проведён:

- анализ типов данных и наличия пропусков;

- исследование распределений и корреляций признаков;

- визуализация выбросов (boxplot).

**Основные наблюдения:**

- Пропуски присутствуют в NumDependents (≈2,5%) и Income (≈20%).

- Распределение классов неравномерное: 93% клиентов без просрочек и 7% с просрочками — сильный дисбаланс.

- Наиболее коррелирующие признаки с целевой переменной:
Num30_59Delinquencies, Num60_89Delinquencies, BalanceToCreditLimit.

**2. Предобработка данных**

**Используемые методы:**

- Замена пропусков медианами (SimpleImputer).

- Масштабирование признаков (StandardScaler).

- Обрезка выбросов по 1-му и 99-му процентилю.

- Исключены неинформативные признаки (client_id).

После предобработки признаки стали сопоставимы по масштабу и очищены от выбросов.

**3. Балансировка и разбиение данных**

Данные разделены в пропорции 80% — обучение, 20% — тест.

Для устранения дисбаланса классов применён **SMOTE** (синтетическое увеличение меньшего класса).
После SMOTE классы сбалансированы (0 и 1 — по 50%).

**4. Модель Decision Tree**

Обучено дерево решений при разных глубинах (от 1 до 11).
Наблюдения:

- При малой глубине модель недообучена.

- При глубине >10 начинается переобучение.

- Оптимальная глубина: 10.

Подбор гиперпараметров через GridSearchCV дал:
- criterion = 'gini'
- max_depth = 12
- min_samples_leaf = 1

**Accuracy: 0.8302**

**5. Бэггинг (BaggingClassifier)**

Базовая модель — логистическая регрессия.

Подбор параметров показал, что лучшие результаты даёт:
- n_estimators = 30
- max_samples = 0.8
- max_features = 0.8
- C = 0.1

**Accuracy: 0.7893**

Вывод: ансамблирование улучшает устойчивость модели, но не даёт максимальной точности в данном случае.

**6. Стекинг (StackingClassifier)**

Базовые модели: KNN, Decision Tree, Logistic Regression.

Финальный классификатор: логистическая регрессия.

Оптимальные параметры:
- dt__max_depth = 2
- knn__n_neighbors = 3
- final_estimator__C = 1

**Accuracy: 0.8575**

Вывод: стекинг улучшил качество за счёт объединения разнородных алгоритмов.

**7. RandomForestClassifier**

После подбора гиперпараметров получено:
- n_estimators = 200
- max_depth = 15
- min_samples_split = 2
- min_samples_leaf = 1

**Accuracy: 0.8551**

Вывод: модель демонстрирует высокую устойчивость и надёжность.

**8. XGBoost**

Параметры модели:
- n_estimators = 200
- max_depth = 5
- learning_rate = 0.1


**Accuracy: 0.8977** — наилучший результат среди всех моделей.

Также проведён **F-test (ANOVA)** для оценки значимости признаков.

Самые важные признаки:

1. Num30_59Delinquencies

2. Num60_89Delinquencies

3. BalanceToCreditLimit

3. Age

4. Income

**9. Сравнение результатов моделей**

| Модель        | Accuracy   |
| ------------- | ---------- |
| **XGBoost**   | **0.8977** |
| Stacking      | 0.8575     |
| Random Forest | 0.8551     |
| Decision Tree | 0.8302     |
| Bagging       | 0.7893     |

**10. Выводы**

1. Проведён полный цикл анализа данных: от EDA до построения и сравнения моделей.

2. Балансировка классов с помощью SMOTE позволила устранить перекос и повысить точность.

3. Среди одиночных моделей дерево решений дало средние результаты, а ансамблевые методы (особенно стекинг и бустинг) значительно улучшили качество.

4. XGBoost показал наилучший результат — точность около 0.90, что подтверждает его эффективность для задач классификации с числовыми признаками.

5. Наиболее важными для прогноза являются признаки, отражающие историю просрочек и кредитную нагрузку клиента.
"""