# -*- coding: utf-8 -*-
"""Lab_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VAcH-n5Ab69pG5bGIcRqBqYTJlDU8MWc

Целью лабораторной работы является разработка и оптимизация модели нейронной сети для прогнозирования рейтинга фильма по данным из реестра прокатных удостоверений и информации о показах фильмов в кинотеатрах.

Задача относится к регрессии, где требуется предсказать числовое значение рейтинга на основе признаков, описывающих фильм (бюджет, страна, режиссёр, жанр, сборы и др.).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Загрузка данных"""

MOVIES_PATH = "/content/drive/MyDrive/Лабораторные работы /Лабораторная 6/mkrf_movies.csv"
SHOWS_PATH = "/content/drive/MyDrive/Лабораторные работы /Лабораторная 6/mkrf_shows.csv"

movies = pd.read_csv(MOVIES_PATH)
shows = pd.read_csv(SHOWS_PATH)
print("movies shape:", movies.shape)
print("shows shape:", shows.shape)

"""Быстрая проверка структуры"""

print("\nmovies.info()")
print(movies.info())
print("\nmovies.describe()")
print(movies.describe(include='all').T.head(30))

print("\nshows.info()")
print(shows.info())
print("\nshows.describe()")
print(shows.describe(include='all').T.head(30))

"""Проверка пропусков, дубликатов, некорректных значений"""

print("\nПропуски в movies (количество):")
print(movies.isna().sum().sort_values(ascending=False).head(30))

print("\nПропуски в shows (количество):")
print(shows.isna().sum().sort_values(ascending=False).head(30))

print("\nДубликаты:")
print("movies duplicated:", movies.duplicated().sum())
print("shows duplicated:", shows.duplicated().sum())

if 'budget' in movies.columns:
    print("budget < 0:", (movies['budget'].astype(str).str.contains('-')).sum())

if 'box_office' in shows.columns:
    print("shows.box_office < 0 (raw):", (pd.to_numeric(shows['box_office'], errors='coerce') < 0).sum())

"""Очистка числовых столбцов"""

def clean_numeric_series(s):
    """Удаляет %, запятые, тире и приводит к float, нечисловые -> NaN"""
    s = s.astype(str).fillna('').str.strip()
    s = s.str.replace('%', '', regex=False)
    s = s.str.replace(',', '.', regex=False)
    s = s.str.replace('—', '', regex=False)
    s = s.str.replace('–', '', regex=False)
    s = s.str.replace('−', '', regex=False)
    s = s.str.replace('None', '', regex=False)
    s = s.str.replace('nan', '', regex=False)
    s = s.str.replace(r'[^\d\.\-]', '', regex=True)
    return pd.to_numeric(s, errors='coerce')

num_cols_movies = []
for c in ['budget', 'refundable_support', 'nonrefundable_support', 'ratings']:
    if c in movies.columns:
        movies[c] = clean_numeric_series(movies[c])
        num_cols_movies.append(c)

if 'box_office' in shows.columns:
    shows['box_office'] = clean_numeric_series(shows['box_office'])

if 'budget' in movies.columns:
    movies['budget'] = movies['budget'].fillna(movies['budget'].median())
if 'ratings' in movies.columns:
    movies['ratings'] = movies['ratings'].fillna(movies['ratings'].median())
for c in ['refundable_support', 'nonrefundable_support']:
    if c in movies.columns:
        movies[c] = movies[c].fillna(0.0)

"""Приведение puNumber к строковому типу"""

movies['puNumber'] = movies['puNumber'].astype(str)
shows['puNumber'] = shows['puNumber'].astype(str)

"""Объединение таблиц: суммарные сборы по puNumber"""

if 'puNumber' in movies.columns and 'puNumber' in shows.columns:

    # Удаляем старый столбец, если есть
    if 'box_office_sum' in movies.columns:
        movies = movies.drop(columns=['box_office_sum'])

    shows_agg = shows.groupby('puNumber', as_index=False)['box_office'].sum()
    shows_agg.rename(columns={'box_office': 'box_office_sum'}, inplace=True)

    movies = movies.merge(shows_agg, on='puNumber', how='left')
    movies['box_office_sum'] = movies['box_office_sum'].fillna(0.0)

    print("\nПосле объединения movies shape:", movies.shape)
else:
    raise RuntimeError("В данных отсутствует колонка 'puNumber' в одной из таблиц.")

"""Обновление списков признаков"""

numerical_features = []

for c in ['budget', 'box_office_sum', 'refundable_support', 'nonrefundable_support', 'ratings']:
    if c in movies.columns:
        numerical_features.append(c)

print("\nЧисловые признаки:", numerical_features)

# Список категориальных признаков
categorical_features = [
    c for c in [
        'type', 'film_studio', 'production_country', 'director', 'producer',
        'age_restriction', 'financing_source', 'genres'
    ] if c in movies.columns
]

print("Категориальные признаки:", categorical_features)

"""EDA"""

plot_num = [c for c in numerical_features if c != 'ratings']
if plot_num:
    movies[plot_num].hist(bins=30, figsize=(12, 4 * len(plot_num)))
    plt.suptitle("Гистограммы числовых признаков", y=1.02)
    plt.show()

plt.figure(figsize=(12, 4))
sns.boxplot(data=movies[plot_num])
plt.title("Boxplot числовых признаков")
plt.show()

for col in categorical_features:
    print(f"\nTop-10 для {col}:")
    print(movies[col].value_counts().head(10))
    plt.figure(figsize=(8,4))
    sns.countplot(y=col, data=movies, order=movies[col].value_counts().iloc[:10].index)
    plt.title(f"Распределение (top 10) - {col}")
    plt.show()

corr_cols = [c for c in numerical_features if c in movies.columns]
if corr_cols:
    plt.figure(figsize=(8, 6))
    corr_mat = movies[corr_cols].corr()
    sns.heatmap(corr_mat, annot=True, fmt=".2f", cmap='coolwarm')
    plt.title("Корреляция числовых признаков")
    plt.show()

"""Логарифмирование skewed признаков"""

if 'budget' in movies.columns:
    movies['budget_log'] = np.log1p(movies['budget'].clip(lower=0))
else:
    movies['budget_log'] = 0.0
if 'box_office_sum' in movies.columns:
    movies['box_office_log'] = np.log1p(movies['box_office_sum'].clip(lower=0))
else:
    movies['box_office_log'] = 0.0

model_num_features = [f for f in ['budget_log', 'box_office_log', 'refundable_support', 'nonrefundable_support'] if f in movies.columns]
print("\nЧисловые признаки для модели (после логарифма):", model_num_features)

"""Масштабирование числовых признаков"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
movies[model_num_features] = scaler.fit_transform(movies[model_num_features])

"""Подготовка категорий для Embedding"""

for c in categorical_features:
    movies[c] = movies[c].fillna('Unknown').astype(str)

category_maps = {}
category_sizes = {}
for c in categorical_features:
    movies[c] = movies[c].astype('category')
    category_maps[c] = dict(enumerate(movies[c].cat.categories))
    movies[f"{c}_idx"] = movies[c].cat.codes.values
    category_sizes[c] = movies[c].nunique()
print("\nРазмеры словарей категорий (category_sizes):", category_sizes)

X_num = movies[model_num_features].values
X_cat = movies[[f"{c}_idx" for c in categorical_features]].values
y = movies['ratings'].values

print("X_num shape:", X_num.shape)
print("X_cat shape:", X_cat.shape)
print("y shape:", y.shape)

"""Train / Val / Test split"""

from sklearn.model_selection import train_test_split
X_num_trval, X_num_test, X_cat_trval, X_cat_test, y_trval, y_test = train_test_split(
    X_num, X_cat, y, test_size=0.15, random_state=42
)
X_num_train, X_num_val, X_cat_train, X_cat_val, y_train, y_val = train_test_split(
    X_num_trval, X_cat_trval, y_trval, test_size=0.1765, random_state=42
)

print("\nTrain/Val/Test shapes:")
print("X_num_train:", X_num_train.shape, "X_cat_train:", X_cat_train.shape, "y_train:", y_train.shape)
print("X_num_val:", X_num_val.shape, "X_cat_val:", X_cat_val.shape, "y_val:", y_val.shape)
print("X_num_test:", X_num_test.shape, "X_cat_test:", X_cat_test.shape, "y_test:", y_test.shape)

"""Построение модели с Embeddings и KerasTuner"""

!pip install keras-tuner --upgrade

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import keras_tuner as kt

def build_model(hp):
    cat_inputs = []
    cat_embeddings = []

    for c in categorical_features:
        vocab_size = category_sizes[c]
        inp = layers.Input(shape=(1,), name=f"{c}_input")
        default_emb = min(50, max(4, (vocab_size + 1)//2))
        emb_dim = hp.Int(f"emb_dim_{c}", min_value=4, max_value=min(64, default_emb), step=4, default=default_emb)
        emb = layers.Embedding(input_dim=vocab_size, output_dim=emb_dim, name=f"emb_{c}")(inp)
        emb = layers.Reshape((emb_dim,))(emb)
        cat_inputs.append(inp)
        cat_embeddings.append(emb)

    num_input = layers.Input(shape=(X_num.shape[1],), name="num_input")
    x = layers.Concatenate()([num_input] + cat_embeddings)

    for i in range(3):
        units = hp.Choice(f"units_{i}", [64, 128, 256])
        activation = hp.Choice("activation", ["relu", "tanh", "elu"])
        x = layers.Dense(units, activation=activation, name=f"dense_{i}")(x)
        if hp.Boolean("batch_norm"):
            x = layers.BatchNormalization(name=f"bn_{i}")(x)
        if hp.Boolean("dropout"):
            rate = hp.Float("dropout_rate", 0.0, 0.5, step=0.1)
            if rate > 0:
                x = layers.Dropout(rate, name=f"drop_{i}")(x)

    output = layers.Dense(1, activation="linear", name="out")(x)

    opt_choice = hp.Choice("optimizer", ["adam", "rmsprop", "adamax"])
    lr = hp.Float("lr", 1e-4, 1e-2, sampling="log", default=1e-3)
    if opt_choice == "adam":
        opt = keras.optimizers.Adam(learning_rate=lr)
    elif opt_choice == "rmsprop":
        opt = keras.optimizers.RMSprop(learning_rate=lr)
    else:
        opt = keras.optimizers.Adamax(learning_rate=lr)

    model = keras.Model(inputs=[num_input] + cat_inputs, outputs=output, name="MovieRatingNN")
    model.compile(optimizer=opt, loss="mse", metrics=[])
    return model

tuner = kt.RandomSearch(
    build_model,
    objective="val_loss",
    max_trials=20,
    executions_per_trial=1,
    directory="kt_movie_dir",
    project_name="movie_rating_emb"
)

def prepare_inputs(X_num_arr, X_cat_arr):
    inputs = [X_num_arr]
    for i in range(X_cat_arr.shape[1]):
        inputs.append(X_cat_arr[:, i])
    return inputs

print("\nЗапуск поиска гиперпараметров (KerasTuner)...")
tuner.search(
    prepare_inputs(X_num_train, X_cat_train), y_train,
    validation_data=(prepare_inputs(X_num_val, X_cat_val), y_val),
    epochs=40,
    batch_size=32,
    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)],
    verbose=1
)

best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]
best_model = tuner.get_best_models(num_models=1)[0]

print("\nЛучшие гиперпараметры (выбор):")
for key in ['activation', 'optimizer', 'lr', 'dropout_rate', 'batch_norm']:
    if best_hp.get(key) is not None:
        print(f"{key}: {best_hp.get(key)}")
for i in range(3):
    print(f"units_{i}: {best_hp.get(f'units_{i}')}")

"""Финальное обучение на Train+Val"""

print("\nФинальное обучение на объединённой Train+Val выборке")

X_num_trainval = np.vstack([X_num_train, X_num_val])
X_cat_trainval = np.vstack([X_cat_train, X_cat_val])
y_trainval = np.hstack([y_train, y_val])

history = best_model.fit(
    prepare_inputs(X_num_trainval, X_cat_trainval),
    y_trainval,
    epochs=100,
    batch_size=32,  # фиксированный размер батча
    validation_split=0.0,
    callbacks=[
        keras.callbacks.EarlyStopping(monitor='loss', patience=8, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=4, min_lr=1e-6)
    ],
    verbose=1
)

"""Оценка на тесте"""

from sklearn.metrics import mean_squared_error

print("\nОценка на тестовой выборке...")
y_pred = best_model.predict(prepare_inputs(X_num_test, X_cat_test)).flatten()
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"RMSE на тестовой выборке: {rmse:.4f}")

"""Визуализации обучения и предсказаний"""

plt.figure(figsize=(10,5))
plt.plot(history.history.get('loss', []), label='Train Loss (MSE)')
plt.xlabel("Эпоха")
plt.ylabel("MSE")
plt.title("Динамика обучения")
plt.legend()
plt.show()

plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.5)
mn = min(y_test.min(), y_pred.min())
mx = max(y_test.max(), y_pred.max())
plt.plot([mn, mx],[mn, mx], 'r--')
plt.xlabel("Реальные рейтинги")
plt.ylabel("Предсказанные рейтинги")
plt.title("Реальные vs Предсказанные")
plt.show()

"""Feature importance (L2-норма эмбеддингов)"""

print("\nИндикация важности категориальных фич — L2-норма эмбеддингов (усреднённая):")
for layer in best_model.layers:
    if layer.name.startswith("emb_"):
        weights = layer.get_weights()[0]
        l2 = np.linalg.norm(weights, axis=1).mean()
        print(f"{layer.name}: avg embed L2 = {l2:.4f}")

import pandas as pd
import numpy as np
import zipfile
from google.colab import files

# Сохраняем числовые данные и категориальные данные в CSV
# Тренировочные данные
pd.DataFrame(X_num_train).to_csv("X_num_train.csv", index=False)
pd.DataFrame(X_cat_train).to_csv("X_cat_train.csv", index=False)
pd.DataFrame(y_train, columns=["target"]).to_csv("y_train.csv", index=False)

# Валидационные данные
pd.DataFrame(X_num_val).to_csv("X_num_val.csv", index=False)
pd.DataFrame(X_cat_val).to_csv("X_cat_val.csv", index=False)
pd.DataFrame(y_val, columns=["target"]).to_csv("y_val.csv", index=False)

# Тестовые данные
pd.DataFrame(X_num_test).to_csv("X_num_test.csv", index=False)
pd.DataFrame(X_cat_test).to_csv("X_cat_test.csv", index=False)
pd.DataFrame(y_test, columns=["target"]).to_csv("y_test.csv", index=False)

# Словарь категориальных размеров
category_sizes_df = pd.DataFrame({
    "category": list(category_sizes.keys()),
    "size": list(category_sizes.values())
})
category_sizes_df.to_csv("category_sizes.csv", index=False)

# Создаем ZIP файл
with zipfile.ZipFile("movie_data_for_nn.zip", "w") as z:
    z.write("X_num_train.csv")
    z.write("X_cat_train.csv")
    z.write("y_train.csv")
    z.write("X_num_val.csv")
    z.write("X_cat_val.csv")
    z.write("y_val.csv")
    z.write("X_num_test.csv")
    z.write("X_cat_test.csv")
    z.write("y_test.csv")
    z.write("category_sizes.csv")

print("Файл movie_data_for_nn.zip создан!")

# Скачать файл
files.download("movie_data_for_nn.zip")

"""Ход работы

1. Анализ данных (EDA)

Загружены две таблицы:

- mkrf_movies — сведения о фильмах (7486 строк, 15 признаков).

- mkrf_shows — сборы фильмов по прокатным удостоверениям (3158 строк, 2 признака).

Проверены типы данных, пропуски и дубликаты:

- Пропуски наблюдались в budget, ratings, producer, genres.

- Дубликаты отсутствуют.

- Аномалий (отрицательных значений бюджета/сборов) не выявлено.

2. Предобработка данных

Числовые признаки очищены и приведены к типу float.

Пропуски:

- budget и ratings — заполнены медианами;

- refundable_support и nonrefundable_support — нулями.

Данные из таблицы shows агрегированы по puNumber и объединены с movies по этому полю.

Добавлены логарифмы признаков: budget_log, box_office_log.

Категориальные признаки закодированы с помощью embedding layers.

Числовые признаки стандартизированы (StandardScaler).

Данные разделены на:

- Train (70%)

- Validation (15%)

- Test (15%)

3. Построение baseline-модели

Использована архитектура нейронной сети с числовыми и категориальными входами.

Для категориальных признаков — встраивания (embeddings).

Для числовых — стандартный входной слой.

Слои: 3 полносвязных скрытых слоя по 256 нейронов.

Функция активации: tanh

Оптимизатор: RMSprop

Learning rate: ≈ 0.004

Dropout: 0.2

Batch Normalization: не использовался.

Функция потерь: MSE

Метрика для сравнения моделей: RMSE

4. Обучение и подбор гиперпараметров

Подбор гиперпараметров выполнялся с помощью Keras Tuner по параметрам:

- Функция активации (relu, tanh, elu)

- Оптимизатор (adam, rmsprop, adamax)

- Наличие Dropout и BatchNorm

- Размер Dropout (0–0.5)

- Learning rate (1e−4 – 1e−2)

- Количество нейронов в слоях (64–256)

Найден лучший набор гиперпараметров:
- activation: tanh
- optimizer: rmsprop
- lr: 0.004
- dropout_rate: 0.2
- batch_norm: False
- units: [256, 256, 256]

5. Результаты обучения

Модель обучалась на объединённой Train+Val выборке (≈6300 образцов).

Потери (MSE) стремительно снижались — с ~24 до <0.02.

График "Динамика обучения" показал быстрое схождение без переобучения.

RMSE на тестовой выборке:
RMSE = 4.59

Разброс предсказаний относительно реальных рейтингов показал адекватную зависимость (см. scatter-график).

6. Анализ важности признаков (через L2-норму эмбеддингов)
| Признак            | Средняя L2-норма |
| ------------------ | ---------------- |
| age_restriction    | **0.80**         |
| type               | 0.54             |
| genres             | 0.43             |
| director           | 0.41             |
| film_studio        | 0.38             |
| production_country | 0.37             |
| financing_source   | 0.35             |
| producer           | 0.30             |

Наиболее значимыми оказались: возрастное ограничение, тип фильма и жанр.

Выводы

- Проведён анализ данных, выполнена очистка и объединение таблиц.

- Создана baseline-модель нейронной сети с числовыми и категориальными входами через embeddings.

- Выполнен подбор гиперпараметров с помощью Keras Tuner.

- Наилучшие результаты показала сеть с активацией tanh, оптимизатором RMSprop, dropout=0.2.

- Финальная модель достигла RMSE ≈ 4.59 на тестовых данных.

- Наибольшее влияние на рейтинг оказывают жанр, тип фильма и возрастное ограничение.

- Модель показала стабильное обучение и высокую способность к аппроксимации рейтингов.
"""