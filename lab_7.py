# -*- coding: utf-8 -*-
"""Copy of Lab_7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tS0trtzAEqJ8Ks8yO9PIJZ3HKkERpl3J

Построение и обучение полносвязной нейронной сети для предсказания рейтинга фильма

Написать простую нейронную сеть и полный пайплайн обучения с нуля, без использования готовых слоёв, функций потерь, оптимизаторов и т.п.
Разработать всё вручную:

- полносвязные слои

- активации

- обратное распространение ошибки (backpropagation)

- функцию потерь

- оптимизацию SGD

- обучение нескольких архитектур

- выбор лучшей модели

- тестирование и визуализацию
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Загружка CSV из lab_6
X_num_train = pd.read_csv("/content/X_num_train.csv").values.astype(np.float32)
X_cat_train = pd.read_csv("/content/X_cat_train.csv").values.astype(np.int64)
y_train = pd.read_csv("/content/y_train.csv").values.reshape(-1).astype(np.float32)

X_num_val = pd.read_csv("/content/X_num_val.csv").values.astype(np.float32)
X_cat_val = pd.read_csv("/content/X_cat_val.csv").values.astype(np.int64)
y_val = pd.read_csv("/content/y_val.csv").values.reshape(-1).astype(np.float32)

X_num_test = pd.read_csv("/content/X_num_test.csv").values.astype(np.float32)
X_cat_test = pd.read_csv("/content/X_cat_test.csv").values.astype(np.int64)
y_test = pd.read_csv("/content/y_test.csv").values.reshape(-1).astype(np.float32)

"""Подготовка категориальных признаков  (one-hot)"""

# Количество уникальных значений в каждой колонке
category_sizes = {
    f"cat_{i}": int(X_cat_train[:, i].max() + 1)
    for i in range(X_cat_train.shape[1])
}

# One-hot вручную
def one_hot(X_cat, category_sizes):
    parts = []
    for col in range(X_cat.shape[1]):
        size = category_sizes[f"cat_{col}"]
        x = X_cat[:, col]
        x = np.minimum(x, size - 1)  # защита от unseen category
        oh = np.zeros((len(x), size), dtype=np.float32)
        oh[np.arange(len(x)), x] = 1
        parts.append(oh)
    return np.concatenate(parts, axis=1)

# Финальные матрицы признаков
X_train = np.concatenate([X_num_train, one_hot(X_cat_train, category_sizes)], axis=1)
X_val   = np.concatenate([X_num_val,   one_hot(X_cat_val,   category_sizes)], axis=1)
X_test  = np.concatenate([X_num_test,  one_hot(X_cat_test,  category_sizes)], axis=1)

print("Форма X_train:", X_train.shape)
print("Форма X_val:", X_val.shape)
print("Форма X_test:", X_test.shape)

"""Базовые слои  (Linear + ReLU)"""

# Полносвязный слой
class Linear:
    def __init__(self, in_f, out_f):
        # Инициализация весов маленькими значениями
        self.W = np.random.randn(in_f, out_f).astype(np.float32) * 0.01
        self.b = np.zeros(out_f, dtype=np.float32)
        self.x = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.x = x
        return x @ self.W + self.b

    def backward(self, grad_out):
        self.dW = self.x.T @ grad_out
        self.db = grad_out.sum(axis=0)
        return grad_out @ self.W.T

    def step(self, lr):
        self.W -= lr * self.dW
        self.b -= lr * self.db

# ReLU активация
class ReLU:
    def __init__(self):
        self.mask = None

    def forward(self, x):
        self.mask = x > 0
        return x * self.mask

    def backward(self, grad_out):
        return grad_out * self.mask

    def step(self, lr):
        pass

"""Архитектура нейронной сети"""

class MyNet:
    def __init__(self, input_dim, h1, h2):
        self.layers = [
            Linear(input_dim, h1),
            ReLU(),
            Linear(h1, h2),
            ReLU(),
            Linear(h2, 1)
        ]

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, grad):
        for layer in reversed(self.layers):
            grad = layer.backward(grad)

    def step(self, lr):
        for layer in self.layers:
            layer.step(lr)

"""Функция потерь  (MSE)"""

def mse_loss(y_pred, y_true):
    loss = np.mean((y_pred - y_true)**2)
    grad = 2 * (y_pred - y_true) / len(y_true)
    return loss, grad

"""Обучение сети"""

def train(model, X_tr, y_tr, X_val, y_val, epochs=100, lr=0.005, batch=64):
    n = len(X_tr)
    for e in range(epochs):
        perm = np.random.permutation(n)
        X_tr = X_tr[perm]
        y_tr = y_tr[perm]

        for i in range(0, n, batch):
            xb = X_tr[i:i+batch]
            yb = y_tr[i:i+batch].reshape(-1, 1)

            pred = model.forward(xb)
            loss, grad = mse_loss(pred, yb)
            model.backward(grad)
            model.step(lr)

        val_pred = model.forward(X_val)
        val_loss, _ = mse_loss(val_pred, y_val.reshape(-1, 1))
        print(f"Epoch {e+1}/{epochs}  val_loss={val_loss:.4f}")

    return val_loss

"""Три модели"""

input_dim = X_train.shape[1]

models = {
    "model_1": MyNet(input_dim, 64, 32),
    "model_2": MyNet(input_dim, 128, 64),
    "model_3": MyNet(input_dim, 256, 128)
}

results = {}

"""Обучение каждой модели"""

for name, model in models.items():
    print("\n===== Обучение", name, "=====")
    result = train(model, X_train, y_train, X_val, y_val,
                   epochs=100, lr=0.005)
    results[name] = result

print("\nРезультаты:", results)

# Выбор лучшей модели
best_model_name = min(results, key=results.get)
best_model = models[best_model_name]

print("\nЛучшая модель:", best_model_name)

"""Тестирование на  test"""

y_pred = best_model.forward(X_test).reshape(-1)

mse = np.mean((y_pred - y_test)**2)
rmse = np.sqrt(mse)

print("\nИтоги на TEST")
print("MSE:", mse)
print("RMSE:", rmse)

# График: реальные vs предсказанные
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, s=8)
plt.plot([y_test.min(), y_test.max()],
         [y_test.min(), y_test.max()], 'r')
plt.xlabel("Истинные значения")
plt.ylabel("Предсказания")
plt.title("Сравнение предсказаний и реальных значений")
plt.show()

"""Ход работы

1. Подготовка данных

Данные из предыдущей лабораторной (lab_6):

- числовые признаки

- категориальные признаки

- целевую переменную

Категориальные признаки преобразованы вручную через one-hot, без sklearn.

Финальная размерность:

X_train: (5239, 17366)

X_val:   (1124, 17366)

X_test:  (1123, 17366)

2. Реализация нейронной сети с нуля

Реализованы вручную:
1) Полносвязный слой (Linear)

- хранит W, b

- прямой проход xW + b

- подсчет градиентов dW, db

- шаг SGD

2) Активация ReLU

- реализована вручную:

ReLu(x)=max(0,x)

Почему ReLU?

- быстро обучается

- не вызывает vanishing gradient

- стандарт для глубоких сетей

3) Функция потерь MSE

- регрессия → MSE — стандарт

- легко дифференцировать

- корректно работает на вещественных данных

4) Backpropagation

Реализован полностью вручную:

- каждый слой получает grad_out

- считает свои градиенты

- передаёт назад

5) Stochastic Gradient Descent (SGD)

- вручную обновляются веса

W -= lr * dW

3. Обучение нескольких моделей

Три архитектуры:

| Модель  | Структура | Результат (val_loss) |
| ------- | --------- | -------------------- |
| model_1 | 64 → 32   | **29.47 (лучшая)**   |
| model_2 | 128 → 64  | 29.95                |
| model_3 | 256 → 128 | 29.71                |

Используется валидацию для выбора лучшей модели — требуется для контроля переобучения.

4. Тестирование лучшей модели

Лучшая модель = model_1

На тесте:

MSE = 20.2178

RMSE = 4.4964

RMSE показывает среднюю ошибку ≈ 4.5 единицы, что соответствует разбросу данных.

5. Визуализация результата

график:

- синяя точка = предсказание

- красная линия = идеал y = x

Модель предсказывает разумно, кластер данных примерно воспроизводится.

Распределение целевой переменной сильно смещено:
подавляющее большинство объектов имеет значения y в диапазоне 0–10, а большие значения встречаются редко (редкие выбросы).

Модель плохо предсказывает большие значения (80–100), потому что:

- редкие значения плохо обучают сеть (их мало → сеть их "игнорирует")

- MSE усиливает влияние маленьких значений, если их большинство

- простая модель + большое количество признаков (17 366)

- веса обновляются на всём наборе, и сеть учится "усреднять" результат

- не делали нормализацию целевой переменной, а масштаб очень разный

Поэтому сеть хорошо запомнила плотный кластер около нуля, но плохо — редкие большие числа.

В лабораторной работе была разработана полноценная нейронная сеть с нуля:

- без использования PyTorch, sklearn, Keras

- вручную реализованы Linear, ReLU, backprop, MSE, SGD

- подготовлены данные, включая one-hot

- обучены 3 архитектуры + выбран лучший вариант

- проведено тестирование

- построен график качества
"""